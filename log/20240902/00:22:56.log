00:22:56 - INFO - The full evaluation configuration:
Namespace(auto_model=False, datapath='/data0/amax/git/MCSD/dataset/wmt_ende.json', disable_tqdm=True, disable_tree_attn=False, draft_model='/data0/lygao/model/llama/vicuna-160m', flash_attn=False, fp16=False, k_config=(3, 2), max_new_tokens=128, naive_sampling=False, replacement=False, run_baseline=False, sampling_type='sampling', target_model='/data0/lygao/model/llama/llama-13b', tokenizer='/data0/lygao/model/llama/llama-13b')
00:22:56 - INFO - Loading draft model: /data0/lygao/model/llama/vicuna-160m
00:22:57 - INFO - Loading target model: /data0/lygao/model/llama/llama-13b
00:22:57 - ERROR - Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
00:22:57 - ERROR - /home/amax/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
00:22:59 - ERROR - Loading checkpoint shards:  33%|###3      | 1/3 [00:02<00:04,  2.37s/it]
00:23:01 - ERROR - Loading checkpoint shards:  67%|######6   | 2/3 [00:04<00:02,  2.13s/it]
00:23:02 - ERROR - Loading checkpoint shards:  67%|######6   | 2/3 [00:05<00:02,  2.69s/it]
00:23:02 - ERROR - Uncaught exception
Traceback (most recent call last):
  File "evaluation_log.py", line 338, in <module>
    main(args)
  File "evaluation_log.py", line 201, in main
    target_model = ModelLoader.from_pretrained(
  File "/home/amax/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/modeling_utils.py", line 3941, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/amax/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/modeling_utils.py", line 4415, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/amax/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/modeling_utils.py", line 936, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/home/amax/miniconda3/envs/pytorch/lib/python3.8/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 136.00 MiB. GPU 7 has a total capacty of 47.54 GiB of which 51.19 MiB is free. Process 3433999 has 22.95 GiB memory in use. Including non-PyTorch memory, this process has 24.51 GiB memory in use. Of the allocated memory 24.24 GiB is allocated by PyTorch, and 19.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
