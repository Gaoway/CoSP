00:25:04 - INFO - The full evaluation configuration:
Namespace(auto_model=False, datapath='/data0/amax/git/MCSD/dataset/wmt_ende.json', disable_tqdm=True, disable_tree_attn=False, draft_model='/data0/lygao/model/llama/vicuna-160m', flash_attn=False, fp16=False, k_config=(3, 2), max_new_tokens=128, naive_sampling=False, replacement=False, run_baseline=False, sampling_type='sampling', target_model='/data0/lygao/model/llama/llama-13b', tokenizer='/data0/lygao/model/llama/llama-13b')
00:25:04 - INFO - Loading draft model: /data0/lygao/model/llama/vicuna-160m
00:25:05 - INFO - Loading target model: /data0/lygao/model/llama/llama-13b
00:25:05 - ERROR - Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
00:25:05 - ERROR - /home/amax/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
00:25:07 - ERROR - Loading checkpoint shards:  33%|###3      | 1/3 [00:02<00:04,  2.30s/it]
00:25:09 - ERROR - Loading checkpoint shards:  67%|######6   | 2/3 [00:04<00:02,  2.09s/it]
00:25:10 - ERROR - Loading checkpoint shards: 100%|##########| 3/3 [00:05<00:00,  1.67s/it]
00:25:10 - ERROR - Loading checkpoint shards: 100%|##########| 3/3 [00:05<00:00,  1.81s/it]
00:25:10 - ERROR - /home/amax/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
00:25:10 - ERROR - /home/amax/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
00:25:10 - ERROR - /home/amax/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
00:25:10 - ERROR - /home/amax/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
00:25:11 - INFO - Arguments: Namespace(auto_model=False, datapath='/data0/amax/git/MCSD/dataset/wmt_ende.json', disable_tqdm=True, disable_tree_attn=False, draft_model='/data0/lygao/model/llama/vicuna-160m', flash_attn=False, fp16=False, k_config=(3, 2), max_new_tokens=128, naive_sampling=False, replacement=False, run_baseline=False, sampling_type='sampling', target_model='/data0/lygao/model/llama/llama-13b', tokenizer='/data0/lygao/model/llama/llama-13b')
00:25:11 - INFO - k_config: 1,1
00:25:11 - INFO - sample_idx: 0, k_config: (1, 1)
00:25:11 - INFO - input_ids size: torch.Size([1, 29]), 
tensor([[    1,  4103,  9632,   278,  1881,  4223, 10541,   964,  5332, 29889,
            13,  4290, 29901, 24032,   496, 29901,   512,  1037,  1463, 15332,
           363,  8939,   342,   374,   550,    13,  6466, 29901, 29871]],
       device='cuda:1')
00:25:12 - INFO - Draft time: 0.821, speed: 26.791
00:25:12 - INFO - Verify time: 0.606, speed: 18.150
00:25:12 - ERROR - Uncaught exception
Traceback (most recent call last):
  File "evaluation_log.py", line 339, in <module>
    main(args)
  File "evaluation_log.py", line 223, in main
    run_eval(
  File "evaluation_log.py", line 97, in run_eval
    output = generator.generate(input_ids)
  File "/data0/amax/git/MCSD/MCSD/inference/generate_log.py", line 209, in generate
    logger.info(f"Generate output: {(input_ids.size()-init_input_len)} / {invocation_count}, ")
TypeError: unsupported operand type(s) for -: 'torch.Size' and 'int'
