The full evaluation configuration:
Namespace(auto_model=False, datapath='/data0/amax/git/MCSD/dataset/wmt_ende.json', disable_tqdm=True, disable_tree_attn=False, draft_model='/data0/lygao/model/llama/vicuna-68m', flash_attn=False, fp16=False, k_config=(3, 2), max_new_tokens=128, naive_sampling=False, replacement=False, run_baseline=False, sampling_type='argmax', target_model='/data0/lygao/model/llama/llama-13b', tokenizer='/data0/lygao/model/llama/llama-13b')
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]
/home/amax/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  33%|###3      | 1/3 [00:02<00:04,  2.27s/it]
Loading checkpoint shards:  67%|######6   | 2/3 [00:04<00:02,  2.24s/it]
Loading checkpoint shards: 100%|##########| 3/3 [00:05<00:00,  1.88s/it]
Loading checkpoint shards: 100%|##########| 3/3 [00:05<00:00,  1.98s/it]
/home/amax/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/amax/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/amax/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/amax/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
samples: 0, invocation_count: 15
samples: 1, invocation_count: 45
samples: 2, invocation_count: 64
samples: 3, invocation_count: 61
samples: 4, invocation_count: 47
Uncaught exception
Traceback (most recent call last):
  File "evaluation_log.py", line 397, in <module>
    main(args)
  File "evaluation_log.py", line 242, in main
    run_baseline_eval(
  File "evaluation_log.py", line 193, in run_baseline_eval
    output = generator.generate(input_ids)
  File "/data0/amax/git/MCSD/MCSD/inference/generate_log.py", line 97, in generate
    self.eos_token_id in input_ids[0, -1:]
  File "/home/amax/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/_tensor.py", line 1059, in __contains__
    return (element == self).any().item()  # type: ignore[union-attr]
KeyboardInterrupt
